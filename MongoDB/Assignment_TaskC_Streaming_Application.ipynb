{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5148 Assignment 2\n",
    "#### Task C2\n",
    "\n",
    "* Due: 24/05/2019\n",
    "* Tutor: Paras Sitoula, Wednesday 12-2pm\n",
    "<br>\n",
    "\n",
    "| Student 1 | Student 2|\n",
    "|-----------|----------|\n",
    "| Hitesh Get | Samuel Campbell |\n",
    "\n",
    "<br>\n",
    "\n",
    "<dl>\n",
    "    <dt><u>Please Note:</u></dt>\n",
    "    <dd>Obviously, please make sure the producers have been run.</dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import geohash as gh\n",
    "from ast import literal_eval \n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendDataToDB(iter):\n",
    "    \"\"\"\n",
    "    Connects to the database and runs through the data.\n",
    "    \n",
    "    First bins the different streams as appropriate.\n",
    "    \n",
    "    Then, iterates through the climate data, joining satellites where valid.\n",
    "    \"\"\"\n",
    "    \n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_db\n",
    "    the_bin = db.the_bin\n",
    "    \n",
    "    list_climate = []\n",
    "    list_aqua = []\n",
    "    list_terra = []\n",
    "    \n",
    "    for record in iter:\n",
    "        data = json.loads(record[1])\n",
    "        \n",
    "        if 'Climate' in data.keys():\n",
    "            data['_id'] = the_bin.count()\n",
    "            list_climate.append(data)\n",
    "        elif 'hotspot_aqua' in data.keys():\n",
    "            list_aqua.append(data)\n",
    "        elif 'hotspot_terra' in data.keys():\n",
    "            list_terra.append(data)\n",
    "            \n",
    "    # If only the climate stream is encountered, write straight to the database\n",
    "    if len(list_aqua) + len(list_terra) == 0:\n",
    "        for climate in list_climate:\n",
    "            try:\n",
    "                the_bin.insert(climate)\n",
    "                #the_bin.insert({'temp': list_temp, 'conf': list_conf})\n",
    "            except Exception as ex:\n",
    "                print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "                \n",
    "    # Otherwise, for each encountered satellite, checks if a join is valid (using geohash)\n",
    "    # If so, bins its surface temperature and confidence\n",
    "    # If a different kind of satellite is encountered, alters those values to the mean of the pair\n",
    "    else:\n",
    "        for climate in list_climate:\n",
    "            climate_geohash = gh.encode(climate['Climate'][\"latitude\"], climate['Climate'][\"longitude\"], precision = 5)\n",
    "            list_temp = []\n",
    "            list_conf = []\n",
    "            hotspot_iter = 0\n",
    "                        \n",
    "            for aqua in list_aqua:\n",
    "                if gh.encode(aqua['hotspot_aqua'][\"latitude\"], aqua['hotspot_aqua'][\"longitude\"], precision = 5) == climate_geohash:\n",
    "                    list_temp.append(aqua['hotspot_aqua']['surface_temperature_celcius'])\n",
    "                    list_conf.append(aqua['hotspot_aqua']['confidence'])\n",
    "                                                                                \n",
    "            for terra in list_terra:\n",
    "                if gh.encode(terra['hotspot_terra'][\"latitude\"], terra['hotspot_terra'][\"longitude\"], precision = 5) == climate_geohash:\n",
    "                    if len(list_temp) == hotspot_iter:\n",
    "                        list_temp.append(terra['hotspot_terra']['surface_temperature_celcius'])\n",
    "                        list_conf.append(terra['hotspot_terra']['confidence'])\n",
    "                    else:\n",
    "                        list_temp[hotspot_iter] = (list_temp[hotspot_iter] + terra['hotspot_terra']['surface_temperature_celcius']) / 2\n",
    "                        list_conf[hotspot_iter] = (list_conf[hotspot_iter] + terra['hotspot_terra']['confidence']) / 2\n",
    "                    \n",
    "                    hotspot_iter += 1\n",
    "            \n",
    "            # Appends hotspot values\n",
    "            for i in range(0, len(list_temp)):\n",
    "                climate['hotspot'].append({\n",
    "                    'surface_temperature_celcius': list_temp[i],\n",
    "                    'confidence': list_conf[i]\n",
    "                })\n",
    "            \n",
    "            # Writes to database\n",
    "            try:\n",
    "                the_bin.insert(climate)\n",
    "                #the_bin.insert({'temp': list_temp, 'conf': list_conf})\n",
    "            except Exception as ex:\n",
    "                print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "                \n",
    "    client.close()\n",
    "\n",
    "# Set window and topic\n",
    "n_secs = 10\n",
    "topic = \"climate\"\n",
    "\n",
    "# Instantiates Spark processes\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf = conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "\n",
    "# Connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'climate-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'earliest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "    \n",
    "# Processes RDDs\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "\n",
    "time.sleep(60) # Run stream for 1 minute just in case no detection of producer\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
